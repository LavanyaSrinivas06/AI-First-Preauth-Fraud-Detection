{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c0fd74a",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "- `pathlib.Path` – to build file paths in a clean, OS-independent way  \n",
    "- `numpy`, `pandas` – to work with numeric data and DataFrames  \n",
    "- `XGBClassifier`, `plot_importance` – the XGBoost model and feature importance plotting  \n",
    "- `sklearn.metrics` – precision, recall, F1, ROC-AUC, confusion matrix, text report  \n",
    "- `RandomizedSearchCV` – to try different XGBoost hyperparameters and pick the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a997da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c34743",
   "metadata": {},
   "source": [
    "## Load processed train / validation / test splits\n",
    "\n",
    "Here we load the preprocessed datasets produced by the Phase 1 pipeline:\n",
    "\n",
    "- `train.csv` – SMOTE-balanced training data after preprocessing  \n",
    "- `val.csv` – validation split (no SMOTE applied)  \n",
    "- `test.csv` – final hold-out test split (no SMOTE applied)  \n",
    "\n",
    "Each row is a transaction, and each column (except `Class`) is a numeric feature:\n",
    "\n",
    "- PCA components from the original dataset  \n",
    "- Engineered behavioral features  \n",
    "- Device / network / geo features  \n",
    "- One-hot encoded categorical variables  \n",
    "\n",
    "The shapes:\n",
    "\n",
    "- Train: **27,942 rows × 7,622 features**  \n",
    "- Val: **3,000 rows × 7,622 features**  \n",
    "- Test: **3,000 rows × 7,622 features**  \n",
    "\n",
    "This confirms the preprocessing pipeline created a wide feature matrix suitable for XGBoost.\n",
    "The target label is the column `Class` (0 = non-fraud, 1 = fraud).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "284bd9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook dir: /Users/lavanyasrinivas/Documents/AI-First-Preauth-Fraud-Detection/AI-First-Preauth-Fraud-Detection/notebooks\n",
      "Project root: /Users/lavanyasrinivas/Documents/AI-First-Preauth-Fraud-Detection/AI-First-Preauth-Fraud-Detection\n",
      "DATA_DIR: /Users/lavanyasrinivas/Documents/AI-First-Preauth-Fraud-Detection/AI-First-Preauth-Fraud-Detection/data/processed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((27942, 7622), (3000, 7622), (3000, 7622))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Identify project root correctly (2 levels up)\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "ROOT = NOTEBOOK_DIR.parents[0]\n",
    "\n",
    "\n",
    "print(\"Notebook dir:\", NOTEBOOK_DIR)\n",
    "print(\"Project root:\", ROOT)\n",
    "\n",
    "DATA_DIR = ROOT / \"data\" / \"processed\"\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "\n",
    "TRAIN_PATH = DATA_DIR / \"train.csv\"\n",
    "VAL_PATH   = DATA_DIR / \"val.csv\"\n",
    "TEST_PATH  = DATA_DIR / \"test.csv\"\n",
    "\n",
    "# Load files\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "val_df = pd.read_csv(VAL_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "\n",
    "train_df.shape, val_df.shape, test_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ab429f",
   "metadata": {},
   "source": [
    "## Train / validation / test label distribution\n",
    "\n",
    "We now split the data into:\n",
    "\n",
    "- `X_*` – all numeric features (after preprocessing and encoding)  \n",
    "- `y_*` – the target label `Class` (0 = non-fraud, 1 = fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3324b3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X: (27942, 7621)  y: (27942,)\n",
      "Val   X: (3000, 7621)  y: (3000,)\n",
      "Test  X: (3000, 7621)  y: (3000,)\n",
      "\n",
      "Class balance (train):\n",
      "Class\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class balance (val):\n",
      "Class\n",
      "0    0.998333\n",
      "1    0.001667\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class balance (test):\n",
      "Class\n",
      "0    0.999333\n",
      "1    0.000667\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "TARGET_COL = \"Class\"\n",
    "\n",
    "# Split into features (X) and labels (y)\n",
    "X_train = train_df.drop(columns=[TARGET_COL])\n",
    "y_train = train_df[TARGET_COL]\n",
    "\n",
    "X_val = val_df.drop(columns=[TARGET_COL])\n",
    "y_val = val_df[TARGET_COL]\n",
    "\n",
    "X_test = test_df.drop(columns=[TARGET_COL])\n",
    "y_test = test_df[TARGET_COL]\n",
    "\n",
    "print(\"Train X:\", X_train.shape, \" y:\", y_train.shape)\n",
    "print(\"Val   X:\", X_val.shape,   \" y:\", y_val.shape)\n",
    "print(\"Test  X:\", X_test.shape,  \" y:\", y_test.shape)\n",
    "\n",
    "print(\"\\nClass balance (train):\")\n",
    "print(y_train.value_counts(normalize=True).rename(\"proportion\"))\n",
    "\n",
    "print(\"\\nClass balance (val):\")\n",
    "print(y_val.value_counts(normalize=True).rename(\"proportion\"))\n",
    "\n",
    "print(\"\\nClass balance (test):\")\n",
    "print(y_test.value_counts(normalize=True).rename(\"proportion\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcab825",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The class proportions are:\n",
    "\n",
    "- **Train:** 50% non-fraud, 50% fraud → this is expected because SMOTE was applied only to the training split to balance the classes.  \n",
    "- **Validation:** ~0.17% fraud → highly imbalanced, closer to real-world fraud rates.  \n",
    "- **Test:** ~0.07% fraud → extremely imbalanced, also realistic.\n",
    "\n",
    "This setup is intentional:\n",
    "\n",
    "- The model is **trained** on balanced data (to learn the minority class better).\n",
    "- The model is **evaluated** on imbalanced validation and test sets, to see how it behaves under realistic fraud conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b4bae",
   "metadata": {},
   "source": [
    "## Baseline XGBoost training\n",
    "\n",
    "Here we train a first XGBoost model on the SMOTE-balanced training data.\n",
    "\n",
    "Important:\n",
    "- `scale_pos_weight` is computed from the class distribution (here it will be ~1.0 because the data is balanced).\n",
    "- `eval_set` in XGBoost uses a list of `(X, y)` pairs, **not** named triples.  \n",
    "  So we pass `[(X_val, y_val)]` instead of `(\"validation\", X_val, y_val)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8351c7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts (train): {0: 13971, 1: 13971}\n",
      "scale_pos_weight: 1.0\n",
      "✅ Baseline XGBoost model trained.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Compute scale_pos_weight from the (balanced) training labels\n",
    "values, counts = np.unique(y_train, return_counts=True)\n",
    "class_counts = dict(zip(values, counts))\n",
    "neg = class_counts.get(0, 0)\n",
    "pos = class_counts.get(1, 0)\n",
    "scale_pos_weight = neg / pos if pos > 0 else 1.0\n",
    "\n",
    "print(\"Class counts (train):\", class_counts)\n",
    "print(\"scale_pos_weight:\", scale_pos_weight)\n",
    "\n",
    "# Baseline XGBoost model\n",
    "baseline_xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    tree_method=\"hist\",     # fast on CPU\n",
    "    eval_metric=\"logloss\",  # avoids warnings\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "baseline_xgb.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_val, y_val)],  # <-- FIXED: list of (X, y) tuples\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(\"✅ Baseline XGBoost model trained.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03d4606",
   "metadata": {},
   "source": [
    "## Evaluate baseline XGBoost on validation and test sets\n",
    "\n",
    "Now that the baseline XGBoost model is trained on the SMOTE-balanced training data,  \n",
    "we evaluate it on the **imbalanced** validation and test splits.\n",
    "\n",
    "For each split we compute:\n",
    "\n",
    "- **Precision** – of all predicted frauds, how many are truly fraud  \n",
    "- **Recall** – of all true frauds, how many we correctly catch  \n",
    "- **F1-score** – harmonic mean of precision and recall (good for imbalanced data)  \n",
    "- **ROC-AUC** – how well the model separates fraud vs non-fraud across thresholds  \n",
    "- **Confusion matrix** – counts of TP, FP, FN, TN  \n",
    "- **Classification report** – per-class precision/recall/F1 and overall averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b9b247e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== VALIDATION EVALUATION =====\n",
      "Precision: 0.8000\n",
      "Recall:    0.8000\n",
      "F1-score:  0.8000\n",
      "ROC-AUC:   0.9876\n",
      "\n",
      "Confusion matrix:\n",
      "[[2994    1]\n",
      " [   1    4]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9997    0.9997    0.9997      2995\n",
      "           1     0.8000    0.8000    0.8000         5\n",
      "\n",
      "    accuracy                         0.9993      3000\n",
      "   macro avg     0.8998    0.8998    0.8998      3000\n",
      "weighted avg     0.9993    0.9993    0.9993      3000\n",
      "\n",
      "\n",
      "===== TEST EVALUATION =====\n",
      "Precision: 1.0000\n",
      "Recall:    1.0000\n",
      "F1-score:  1.0000\n",
      "ROC-AUC:   1.0000\n",
      "\n",
      "Confusion matrix:\n",
      "[[2998    0]\n",
      " [   0    2]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      2998\n",
      "           1     1.0000    1.0000    1.0000         2\n",
      "\n",
      "    accuracy                         1.0000      3000\n",
      "   macro avg     1.0000    1.0000    1.0000      3000\n",
      "weighted avg     1.0000    1.0000    1.0000      3000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'precision': 0.8, 'recall': 0.8, 'f1': 0.8, 'roc_auc': 0.9876460767946578},\n",
       " {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'roc_auc': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "def evaluate_split(model, X, y, split_name: str):\n",
    "    print(f\"\\n===== {split_name.upper()} EVALUATION =====\")\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X)[:, 1]\n",
    "    else:\n",
    "        y_proba = y_pred  # fallback, not ideal but safe\n",
    "\n",
    "    # Binary metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y, y_proba)\n",
    "    except ValueError:\n",
    "        roc_auc = float(\"nan\")\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    print(confusion_matrix(y, y_pred))\n",
    "\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y, y_pred, digits=4))\n",
    "\n",
    "    return {\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "        \"roc_auc\": float(roc_auc),\n",
    "    }\n",
    "\n",
    "# Evaluate on validation and test\n",
    "val_metrics_baseline = evaluate_split(baseline_xgb, X_val, y_val, split_name=\"validation\")\n",
    "test_metrics_baseline = evaluate_split(baseline_xgb, X_test, y_test, split_name=\"test\")\n",
    "\n",
    "val_metrics_baseline, test_metrics_baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a51278d",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with RandomizedSearchCV\n",
    "\n",
    "The baseline XGBoost model already performs well (F1 ≈ 0.80 on validation, ROC-AUC ≈ 0.99).\n",
    "To push performance further, we use `RandomizedSearchCV` to explore a small hyperparameter space:\n",
    "\n",
    "- `max_depth` – tree depth (controls model complexity)  \n",
    "- `learning_rate` – step size for boosting  \n",
    "- `subsample` – fraction of rows used per boosting round  \n",
    "- `colsample_bytree` – fraction of features used per tree  \n",
    "- `n_estimators` – number of boosting trees  \n",
    "\n",
    "We optimize using **F1-score** on cross-validation, since fraud detection cares about both precision and recall.\n",
    "We keep the search space and number of iterations small to avoid very slow training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eca28def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search training shape: (27942, 7621)\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "\n",
      "✅ Hyperparameter search finished.\n",
      "Best params: {'subsample': 0.9, 'n_estimators': 200, 'max_depth': 4, 'learning_rate': 0.03, 'colsample_bytree': 0.8}\n",
      "Best CV F1: 0.9998\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Parameter search space (small but meaningful)\n",
    "param_distributions = {\n",
    "    \"max_depth\": [4, 6, 8],\n",
    "    \"learning_rate\": [0.01, 0.03, 0.05, 0.1],\n",
    "    \"subsample\": [0.7, 0.8, 0.9],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 0.9],\n",
    "    \"n_estimators\": [200, 300, 400],\n",
    "}\n",
    "\n",
    "# Rebuild base model (same as baseline, but will be tuned)\n",
    "base_xgb_for_search = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Optional: if training set becomes very large in future, we could subsample for tuning\n",
    "X_train_search = X_train\n",
    "y_train_search = y_train\n",
    "\n",
    "print(\"Search training shape:\", X_train_search.shape)\n",
    "\n",
    "# RandomizedSearchCV for F1-score\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=base_xgb_for_search,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=10,          # number of parameter combinations to try\n",
    "    scoring=\"f1\",       # optimize F1 for fraud class\n",
    "    cv=3,               # 3-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "search.fit(X_train_search, y_train_search)\n",
    "\n",
    "print(\"\\n✅ Hyperparameter search finished.\")\n",
    "print(\"Best params:\", search.best_params_)\n",
    "print(f\"Best CV F1: {search.best_score_:.4f}\")\n",
    "\n",
    "best_xgb = search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b96754",
   "metadata": {},
   "source": [
    "## Evaluate the tuned XGBoost model\n",
    "\n",
    "Now that RandomizedSearchCV returned the best hyperparameters,  \n",
    "we evaluate the tuned model on the **validation** and **test** sets.\n",
    "\n",
    "This shows how well the tuned model generalizes to imbalanced real-world data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ac0d794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Tuned Model on Validation and Test ===\n",
      "\n",
      "===== VALIDATION (TUNED) EVALUATION =====\n",
      "Precision: 0.8000\n",
      "Recall:    0.8000\n",
      "F1-score:  0.8000\n",
      "ROC-AUC:   0.9958\n",
      "\n",
      "Confusion matrix:\n",
      "[[2994    1]\n",
      " [   1    4]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9997    0.9997    0.9997      2995\n",
      "           1     0.8000    0.8000    0.8000         5\n",
      "\n",
      "    accuracy                         0.9993      3000\n",
      "   macro avg     0.8998    0.8998    0.8998      3000\n",
      "weighted avg     0.9993    0.9993    0.9993      3000\n",
      "\n",
      "\n",
      "===== TEST (TUNED) EVALUATION =====\n",
      "Precision: 1.0000\n",
      "Recall:    1.0000\n",
      "F1-score:  1.0000\n",
      "ROC-AUC:   1.0000\n",
      "\n",
      "Confusion matrix:\n",
      "[[2998    0]\n",
      " [   0    2]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      2998\n",
      "           1     1.0000    1.0000    1.0000         2\n",
      "\n",
      "    accuracy                         1.0000      3000\n",
      "   macro avg     1.0000    1.0000    1.0000      3000\n",
      "weighted avg     1.0000    1.0000    1.0000      3000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'precision': 0.8, 'recall': 0.8, 'f1': 0.8, 'roc_auc': 0.9957929883138564},\n",
       " {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'roc_auc': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n=== Evaluating Tuned Model on Validation and Test ===\")\n",
    "\n",
    "val_metrics_tuned = evaluate_split(best_xgb, X_val, y_val, split_name=\"validation (tuned)\")\n",
    "test_metrics_tuned = evaluate_split(best_xgb, X_test, y_test, split_name=\"test (tuned)\")\n",
    "\n",
    "val_metrics_tuned, test_metrics_tuned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6814b1d",
   "metadata": {},
   "source": [
    "## Threshold tuning for maximum F1-score\n",
    "\n",
    "XGBoost outputs probabilities, but the default decision threshold (0.50) is not optimal for imbalanced fraud data.\n",
    "We search across thresholds from 0.01 to 0.99 and compute F1-score at each threshold.\n",
    "The threshold that gives the highest F1 is selected as the optimal operating point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fa55189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.67\n",
      "Best F1 score: 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Get validation probabilities (not labels)\n",
    "val_proba = best_xgb.predict_proba(X_val)[:, 1]\n",
    "\n",
    "thresholds = np.linspace(0.01, 0.99, 99)\n",
    "f1_scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    preds = (val_proba >= t).astype(int)\n",
    "    f1_scores.append(f1_score(y_val, preds, zero_division=0))\n",
    "\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "print(\"Best threshold:\", best_threshold)\n",
    "print(\"Best F1 score:\", best_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1128f744",
   "metadata": {},
   "source": [
    "## Final evaluation using the tuned probability threshold\n",
    "\n",
    "Here we apply the optimal threshold (found via threshold search) to convert predicted\n",
    "probabilities into fraud labels. This final evaluation gives the true F1, precision,\n",
    "recall, and confusion matrix for the tuned model at its best operating point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9728307e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FINAL EVALUATION (VALIDATION) =====\n",
      "Threshold: 0.6700\n",
      "Precision: 1.0000\n",
      "Recall:    0.8000\n",
      "F1-score:  0.8889\n",
      "\n",
      "Confusion matrix:\n",
      "[[2995    0]\n",
      " [   1    4]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9997    1.0000    0.9998      2995\n",
      "           1     1.0000    0.8000    0.8889         5\n",
      "\n",
      "    accuracy                         0.9997      3000\n",
      "   macro avg     0.9998    0.9000    0.9444      3000\n",
      "weighted avg     0.9997    0.9997    0.9996      3000\n",
      "\n",
      "\n",
      "===== FINAL EVALUATION (TEST) =====\n",
      "Threshold: 0.6700\n",
      "Precision: 1.0000\n",
      "Recall:    1.0000\n",
      "F1-score:  1.0000\n",
      "\n",
      "Confusion matrix:\n",
      "[[2998    0]\n",
      " [   0    2]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000      2998\n",
      "           1     1.0000    1.0000    1.0000         2\n",
      "\n",
      "    accuracy                         1.0000      3000\n",
      "   macro avg     1.0000    1.0000    1.0000      3000\n",
      "weighted avg     1.0000    1.0000    1.0000      3000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'precision': 1.0, 'recall': 0.8, 'f1': 0.8888888888888888},\n",
       " {'precision': 1.0, 'recall': 1.0, 'f1': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_with_threshold(model, X, y, threshold, split_name=\"validation\"):\n",
    "    print(f\"\\n===== FINAL EVALUATION ({split_name.upper()}) =====\")\n",
    "    \n",
    "    proba = model.predict_proba(X)[:, 1]\n",
    "    preds = (proba >= threshold).astype(int)\n",
    "\n",
    "    precision = precision_score(y, preds, zero_division=0)\n",
    "    recall = recall_score(y, preds, zero_division=0)\n",
    "    f1 = f1_score(y, preds, zero_division=0)\n",
    "\n",
    "    print(f\"Threshold: {threshold:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    print(confusion_matrix(y, preds))\n",
    "\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y, preds, digits=4))\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "\n",
    "# Evaluate on validation and test\n",
    "final_val_metrics = evaluate_with_threshold(best_xgb, X_val, y_val, best_threshold, split_name=\"validation\")\n",
    "final_test_metrics = evaluate_with_threshold(best_xgb, X_test, y_test, best_threshold, split_name=\"test\")\n",
    "\n",
    "final_val_metrics, final_test_metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
